{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras==2.3.1\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (1.5.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.3.1) (2.10.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras.backend as K\n",
    "\n",
    "from data import DATA_SET_DIR\n",
    "from elmo.lm_generator import LMDataGenerator\n",
    "from elmo.model import ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "with open('./data/datasets/txt/advertiser_id.all.tokens', 'r') as f:\n",
    "    for i in tqdm(f.readlines()):\n",
    "        data.append(i[:-1])\n",
    "print('划分数据集')\n",
    "with open('./data/datasets/txt/advertiser_id.train.tokens', 'w') as f:\n",
    "    for i in tqdm(data[:1750000]):\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "with open('./data/datasets/txt/advertiser_id.valid.tokens', 'w') as f:\n",
    "    for i in tqdm(data[1750000:2000000]):\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "with open('./data/datasets/txt/advertiser_id.test.tokens', 'w') as f:\n",
    "    for i in tqdm(data[2000000:]):\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用验证、训练数据制作词表\n",
    "data = []\n",
    "with open('./data/datasets/txt/advertiser_id.train.tokens', 'r') as f:\n",
    "    for i in tqdm(f.readlines()):\n",
    "        data.append(i[:-1])\n",
    "with open('./data/datasets/txt/advertiser_id.valid.tokens', 'r') as f:\n",
    "    for i in tqdm(f.readlines()):\n",
    "        data.append(i[:-1])\n",
    "\n",
    "words = []\n",
    "for i in data:\n",
    "    words += i.split(' ')\n",
    "words = set(words) - set(('<unk>',))\n",
    "vocab = {}\n",
    "vocab['<pad>'] = 0\n",
    "vocab['<bos>'] = 1\n",
    "vocab['<eos>'] = 2\n",
    "vocab['<unk>'] = 3\n",
    "i = 0\n",
    "for i, word in tqdm(enumerate(words)):\n",
    "    vocab[word] = i + 4\n",
    "with open('./data/datasets/txt/advertiser_id.vocab', 'w') as f:\n",
    "    for i in tqdm(vocab):\n",
    "        f.write('{} {}\\n'.format(i, vocab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\n",
    "    'multi_processing': True,\n",
    "    'n_threads': os.cpu_count(),\n",
    "    'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False,\n",
    "    'train_dataset': 'txt/advertiser_id.train.tokens',\n",
    "    'valid_dataset': 'txt/advertiser_id.valid.tokens',\n",
    "    'test_dataset': 'txt/advertiser_id.test.tokens',\n",
    "    'vocab': 'txt/advertiser_id.vocab',\n",
    "    'vocab_size': 54837,\n",
    "    'num_sampled': 1000,\n",
    "    'charset_size': 262,\n",
    "    'sentence_maxlen': 100,\n",
    "    'token_maxlen': 50,\n",
    "    'token_encoding': 'word',\n",
    "    'epochs': 10,\n",
    "    'patience': 2,\n",
    "    'batch_size': 16,\n",
    "    'clip_value': 1,\n",
    "    'cell_clip': 5,\n",
    "    'proj_clip': 5,\n",
    "    'lr': 0.2,\n",
    "    'shuffle': True,\n",
    "    'n_lstm_layers': 2,\n",
    "    'n_highway_layers': 2,\n",
    "    'cnn_filters': [[1, 32],\n",
    "                    [2, 32],\n",
    "                    [3, 64],\n",
    "                    [4, 128],\n",
    "                    [5, 256],\n",
    "                    [6, 512],\n",
    "                    [7, 512]\n",
    "                    ],\n",
    "    'lstm_units_size': 400,\n",
    "    'hidden_units_size': 200,\n",
    "    'char_embedding_size': 16,\n",
    "    'dropout_rate': 0.1,\n",
    "    'word_dropout_rate': 0.05,\n",
    "    'weight_tying': True,\n",
    "}\n",
    "\n",
    "# Set-up Generators\n",
    "train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding'])\n",
    "\n",
    "val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# Compile ELMo\n",
    "print('compile')\n",
    "elmo_model = ELMo(parameters)\n",
    "elmo_model.compile_elmo(print_summary=True)\n",
    "\n",
    "# Train ELMo\n",
    "print('train')\n",
    "elmo_model.train(train_data=train_generator, valid_data=val_generator)\n",
    "\n",
    "# Persist ELMo Bidirectional Language Model in disk\n",
    "print('save')\n",
    "elmo_model.save(sampled_softmax=False)\n",
    "\n",
    "# Evaluate Bidirectional Language Model\n",
    "print('evaluate')\n",
    "elmo_model.evaluate(test_generator)\n",
    "\n",
    "# Build ELMo meta-model to deploy for production and persist in disk\n",
    "print('??')\n",
    "elmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)\n",
    "\n",
    "# Load ELMo encoder\n",
    "print('load')\n",
    "elmo_model.load_elmo_encoder()\n",
    "\n",
    "# Get ELMo embeddings to feed as inputs for downstream tasks\n",
    "elmo_embeddings = elmo_model.get_outputs(test_generator, output_type='word', state='mean')\n",
    "\n",
    "# BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G., TEXT CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_indices (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_encoding (Embedding)      (None, None, 200)    11208800    word_indices[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_6 (SpatialDro (None, None, 200)    0           token_encoding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "timestep_dropout_2 (TimestepDro (None, None, 200)    0           spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 200)    0           timestep_dropout_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_7 (CuDNNLSTM)        (None, None, 400)    963200      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 200)    0           spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_5 (CuDNNLSTM)        (None, None, 400)    963200      timestep_dropout_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_7 (Camouflage)       (None, None, 400)    0           cu_dnnlstm_7[0][0]               \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_5 (Camouflage)       (None, None, 400)    0           cu_dnnlstm_5[0][0]               \n",
      "                                                                 spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 200)    80200       camouflage_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 200)    80200       camouflage_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "b_block_1 (Add)                 (None, None, 200)    0           time_distributed_7[0][0]         \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "f_block_1 (Add)                 (None, None, 200)    0           time_distributed_5[0][0]         \n",
      "                                                                 timestep_dropout_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_9 (SpatialDro (None, None, 200)    0           b_block_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, None, 200)    0           f_block_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_8 (CuDNNLSTM)        (None, None, 400)    963200      spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)        (None, None, 400)    963200      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_8 (Camouflage)       (None, None, 400)    0           cu_dnnlstm_8[0][0]               \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "camouflage_6 (Camouflage)       (None, None, 400)    0           cu_dnnlstm_6[0][0]               \n",
      "                                                                 spatial_dropout1d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, None, 200)    80200       camouflage_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 200)    80200       camouflage_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "b_block_2 (Add)                 (None, None, 200)    0           time_distributed_8[0][0]         \n",
      "                                                                 spatial_dropout1d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "f_block_2 (Add)                 (None, None, 200)    0           time_distributed_6[0][0]         \n",
      "                                                                 spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_10 (SpatialDr (None, None, 200)    0           b_block_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, None, 200)    0           f_block_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "next_ids (InputLayer)           (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reverse (Lambda)                (None, None, 200)    0           spatial_dropout1d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "previous_ids (InputLayer)       (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sampled_softmax_2 (SampledSoftm (None, None, 200)    11264844    spatial_dropout1d_8[0][0]        \n",
      "                                                                 next_ids[0][0]                   \n",
      "                                                                 reverse[0][0]                    \n",
      "                                                                 previous_ids[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 15,438,444\n",
      "Trainable params: 15,438,444\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/3\n",
      " 12018/156250 [=>............................] - ETA: 8:11:10 - loss: 1.6729"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    'multi_processing': False,\n",
    "    'n_threads': 4,\n",
    "    'cuDNN': True if len(K.tensorflow_backend._get_available_gpus()) else False,\n",
    "    'train_dataset': 'txt/advertiser_id.demo.tokens',\n",
    "    'valid_dataset': 'txt/advertiser_id.demo.tokens',\n",
    "    'test_dataset': 'txt/advertiser_id.demo.tokens',\n",
    "    'vocab': 'txt/advertiser_id.demo.vocab',\n",
    "    'vocab_size': len(vocab),\n",
    "    'num_sampled': 1000,\n",
    "    'charset_size': 262,\n",
    "    'sentence_maxlen': 100,\n",
    "    'token_maxlen': 50,\n",
    "    'token_encoding': 'word',\n",
    "    'epochs': 10,\n",
    "    'patience': 2,\n",
    "    'batch_size': 1,\n",
    "    'clip_value': 1,\n",
    "    'cell_clip': 5,\n",
    "    'proj_clip': 5,\n",
    "    'lr': 0.2,\n",
    "    'shuffle': True,\n",
    "    'n_lstm_layers': 2,\n",
    "    'n_highway_layers': 2,\n",
    "    'cnn_filters': [[1, 32],\n",
    "                    [2, 32],\n",
    "                    [3, 64],\n",
    "                    [4, 128],\n",
    "                    [5, 256],\n",
    "                    [6, 512],\n",
    "                    [7, 512]\n",
    "                    ],\n",
    "    'lstm_units_size': 400,\n",
    "    'hidden_units_size': 200,\n",
    "    'char_embedding_size': 16,\n",
    "    'dropout_rate': 0.1,\n",
    "    'word_dropout_rate': 0.05,\n",
    "    'weight_tying': True,\n",
    "}\n",
    "\n",
    "# Set-up Generators\n",
    "train_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['train_dataset']),\n",
    "                                  os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                  sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                  token_maxlen=parameters['token_maxlen'],\n",
    "                                  batch_size=parameters['batch_size'],\n",
    "                                  shuffle=parameters['shuffle'],\n",
    "                                  token_encoding=parameters['token_encoding'])\n",
    "\n",
    "val_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['valid_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "test_generator = LMDataGenerator(os.path.join(DATA_SET_DIR, parameters['test_dataset']),\n",
    "                                os.path.join(DATA_SET_DIR, parameters['vocab']),\n",
    "                                sentence_maxlen=parameters['sentence_maxlen'],\n",
    "                                token_maxlen=parameters['token_maxlen'],\n",
    "                                batch_size=parameters['batch_size'],\n",
    "                                shuffle=parameters['shuffle'],\n",
    "                                token_encoding=parameters['token_encoding'])\n",
    "\n",
    "# Compile ELMo\n",
    "print('compile')\n",
    "elmo_model = ELMo(parameters)\n",
    "elmo_model.compile_elmo(print_summary=True)\n",
    "\n",
    "# Train ELMo\n",
    "print('train')\n",
    "elmo_model.train(train_data=train_generator, valid_data=val_generator)\n",
    "\n",
    "# Persist ELMo Bidirectional Language Model in disk\n",
    "print('save')\n",
    "elmo_model.save(sampled_softmax=False)\n",
    "\n",
    "# Evaluate Bidirectional Language Model\n",
    "print('evaluate')\n",
    "elmo_model.evaluate(test_generator)\n",
    "\n",
    "# Build ELMo meta-model to deploy for production and persist in disk\n",
    "print('??')\n",
    "elmo_model.wrap_multi_elmo_encoder(print_summary=True, save=True)\n",
    "\n",
    "# Load ELMo encoder\n",
    "print('load')\n",
    "elmo_model.load_elmo_encoder()\n",
    "\n",
    "# Get ELMo embeddings to feed as inputs for downstream tasks\n",
    "elmo_embeddings = elmo_model.get_outputs(test_generator, output_type='word', state='mean')\n",
    "\n",
    "# BUILD & TRAIN NEW KERAS MODEL FOR DOWNSTREAM TASK (E.G., TEXT CLASSIFICATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
